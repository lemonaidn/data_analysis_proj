violent_crime_counties <-
summarise(check_grand_total = (rape + robbery_agg_assault + breaking_and_entering + larceny_theft + motor_vehicle_theft))
violent_crime_counties |>
summarise(check_grand_total = (murder + rape + robbery_agg_assault + breaking_and_entering + larceny_theft + motor_vehicle_theft))
violent_crime_counties |>
summarise(check_grand_total = (murder+rape+robbery+agg_assault+breaking_and_entering+larceny_theft+motor_vehicle_theft))
violent_crime_counties |>
group_by(county, year)
summarise(check_grand_total = (murder+rape+robbery+agg_assault+breaking_and_entering+larceny_theft+motor_vehicle_theft))
violent_crime_counties |>
group_by(county, year)|>
summarise(check_grand_total = (murder+rape+robbery+agg_assault+breaking_and_entering+larceny_theft+motor_vehicle_theft))
violent_crime_counties |>
group_by(county, year, grand_total)|>
summarise(check_grand_total = (murder+rape+robbery+agg_assault+breaking_and_entering+larceny_theft+motor_vehicle_theft))
violent_crime_counties |>
group_by(county, year, breaking_and_entering_per_100_000_people)|>
summarise(check_breaking_and_entering_per_100_000_people = (population/100000)*breaking_and_entering)
violent_crime_counties |>
group_by(county, year, breaking_and_entering_per_100_000_people)|>
summarise(check_breaking_and_entering_per_100_000_people = (population/breaking_and_entering)*100000)
violent_crime_counties |>
group_by(county, year, breaking_and_entering_per_100_000_people)|>
summarise(check_breaking_and_entering_per_100_000_people = (breaking_and_entering/population)*100000)
violent_crime_counties |>
group_by(county, year, property_crime_percent_change)|>
summarise(check_property_crime_percent_change = fv_percent_diff(year-(year-1))
violent_crime_counties |>
violent_crime_counties$year[-1],property_crime_totals
violent_crime_counties |>
group_by(county, year, breaking_and_entering_per_100_000_people)|>
summarise(percent_change = (property_crime_totals - lag(property_crime_totals)) / lag(property_crime_totals) * 100)
violent_crime_counties |>
group_by(county, year, property_crime_percent_change)|>
summarise(percent_change = (property_crime_totals - lag(property_crime_totals)) / lag(property_crime_totals) * 100)
violent_crime_counties <- violent_crime_counties %>%
group_by(county) %>%
mutate(percent_change = (property_crime_totals - lag(property_crime_totals)) / lag(property_crime_totals) * 100)
print(violent_crime_counties)
violent_crime_counties <- violent_crime_counties %>%
group_by(county) %>%
mutate(percent_change = (property_crime_totals - lag(property_crime_totals)) / lag(property_crime_totals) * 100)
violent_crime_counties <- violent_crime_counties %>%
group_by(county) %>%
mutate(percent_change = (property_crime_totals - lag(property_crime_totals)) / lag(property_crime_totals) * 100)
violent_crime_counties |>
group_by(county, year, property_crime_percent_change)|>
summarise(percent_change = (property_crime_totals - lag(property_crime_totals)) / lag(property_crime_totals) * 100)
violent_crime_counties |>
group_by(year, county,property_crime_percent_change)|>
summarise(percent_change = (property_crime_totals - lag(property_crime_totals)) / lag(property_crime_totals) * 100)
violent_crime_counties |>
arrange(county, year, property_crime_percent_change)
group_by(year)|>
summarise(percent_change = (property_crime_totals - lag(property_crime_totals)) / lag(property_crime_totals) * 100)
violent_crime_counties |>
arrange(county, year, property_crime_percent_change)|>
group_by(year)|>
summarise(percent_change = (property_crime_totals - lag(property_crime_totals)) / lag(property_crime_totals) * 100)
violent_crime_counties |>
arrange(county, year, property_crime_percent_change)|>
group_by(county)|>
summarise(percent_change = (property_crime_totals - lag(property_crime_totals)) / lag(property_crime_totals) * 100)
violent_crime_counties |>
group_by(county, year, property_crime_percent_change)|>
summarise(percent_change = (property_crime_totals - lag(property_crime_totals)) / lag(property_crime_totals) * 100)
# Check property_crime_percent_change using pct_change=((new-old)/old)*100
violent_crime_counties |>
group_by(county, year, property_crime_percent_change)|>
summarise(check_property_crime_percent_change = (property_crime_totals - lag(property_crime_totals)) / lag(property_crime_totals) * 100)
violent_crime_counties %>%
arrange(county, year) %>%
group_by(county) %>%
summarise(year = year,
property_crime_totals = list(property_crime_totals),
percent_change = (property_crime_totals - lag(property_crime_totals)) / lag(property_crime_totals) * 100)
violent_crime_counties |>
group_by(county, year, property_crime_percent_change)|>
summarise(check_property_crime_percent_change = (property_crime_totals - lag(property_crime_totals)) / lag(property_crime_totals) * 100)
violent_crime_counties |>
group_by(county, year, breaking_and_entering_per_100_000_people)|>
summarise(check_breaking_and_entering_per_100_000_people = (breaking_and_entering/population)*100000)
knitr::opts_chunk$set(echo = TRUE)
# Turn off scientific notation.
options(scipen=999)
# Load libraries.
library(tidyverse)
library(lubridate)
library(janitor)
library(tidycensus)
# Load required data.
violent_crime_counties <- read_csv("data/violent_crime_counties.csv")
# Clean required data and prepare for analysis if needed.
violent_crime_counties <- clean_names(violent_crime_counties)
violent_crime_counties <- violent_crime_counties |>
rename(breaking_and_entering = b_e)|>
rename(motor_vehicle_theft = m_v_theft)|>
rename(breaking_and_entering_per_100_000_people = b_e_per_100_000_people)|>
rename(motor_vehicle_theft_per_100_000_people = m_v_theft_per_100_000_people)|>
rename(breaking_and_entering_rate_percent_change_per_100_000_people = b_e_rate_percent_change_per_100_000_people) |>
rename(motor_vehicle_theft_rate_percent_change_per_100_000_people = m_v_theft_rate_percent_change_per_100_000_people) |>
rename(county = jurisdiction)
# Find NA values
sum(is.na(violent_crime_counties))
violent_crime_counties |>
group_by(county, year, grand_total)|>
summarise(check_grand_total = (murder+rape+robbery+agg_assault+breaking_and_entering+larceny_theft+motor_vehicle_theft))
knitr::opts_chunk$set(echo = TRUE)
# Turn off scientific notation.
options(scipen=999)
# Load libraries.
library(tidyverse)
library(lubridate)
library(janitor)
library(tidycensus)
# Load required data.
violent_crime_counties <- read_csv("data/violent_crime_counties.csv")
# Clean required data and prepare for analysis if needed.
violent_crime_counties <- clean_names(violent_crime_counties)
violent_crime_counties <- violent_crime_counties |>
rename(breaking_and_entering = b_e)|>
rename(motor_vehicle_theft = m_v_theft)|>
rename(breaking_and_entering_per_100_000_people = b_e_per_100_000_people)|>
rename(motor_vehicle_theft_per_100_000_people = m_v_theft_per_100_000_people)|>
rename(breaking_and_entering_rate_percent_change_per_100_000_people = b_e_rate_percent_change_per_100_000_people) |>
rename(motor_vehicle_theft_rate_percent_change_per_100_000_people = m_v_theft_rate_percent_change_per_100_000_people) |>
rename(county = jurisdiction)
# Find NA values
sum(is.na(violent_crime_counties))
# Check grand_total column
violent_crime_counties |>
group_by(county, year, grand_total)|>
summarise(check_grand_total = (murder+rape+robbery+agg_assault+breaking_and_entering+larceny_theft+motor_vehicle_theft))
violent_crime_counties |>
group_by(county, year, grand_total)|>
summarise(check_grand_total = (murder+rape+robbery+agg_assault+breaking_and_entering+larceny_theft+motor_vehicle_theft))
violent_crime_counties |>
arrange(county, year, grand_total)|>
summarise(check_grand_total = (murder+rape+robbery+agg_assault+breaking_and_entering+larceny_theft+motor_vehicle_theft))
violent_crime_counties %>%
group_by(county) %>%
arrange(county, year, property_crime_percent_change) %>%
mutate(check_property_crime_percent_change = (property_crime_totals - lag(property_crime_totals)) / lag(property_crime_totals) * 100) %>%
filter(!is.na(check_property_crime_percent_change)) |>
select(county, year, property_crime_percent_change, check_property_crime_percent_change )
violent_crime_counties %>%
group_by(county) %>%
arrange(county, year, property_crime_percent_change) %>%
summarise(check_property_crime_percent_change = (property_crime_totals - lag(property_crime_totals)) / lag(property_crime_totals) * 100) %>%
filter(!is.na(check_property_crime_percent_change)) |>
select(county, year, property_crime_percent_change, check_property_crime_percent_change )
knitr::opts_chunk$set(echo = TRUE)
# Turn off scientific notation.
options(scipen=999)
# Load libraries.
library(tidyverse)
library(readr)
library(scales)
library(janitor)
# Load the data from ProPublica.
# Watch out for "subtotal" rows -- need to filter those out when doing calculations to avoid doubling the real total
# "DATE" column seems to pertain to the date the expense was filed, whereas the start and end date cols seem to pertain to the dates of the expenses themselves. for example, q4_2019 has expenses that go back all the way to 2011
# YEAR is not always populated. When it is, it seems to pertain to the filing year rather than the expense year.
# QUARTER is sometimes unpopulated, sometimes populated with a numeric, and sometimes populated with a chr (eg "Q2" instead of just "2").
# DATE col values aren't always populated (within the same df some rows are and some rows aren't). Can't use it to extract quarter and year values.
# Cleaned datatypes in columns one-by-one while exploring the distinct issues across different quarters. We will make this more concise in the near future by using regex and a for loop to iterate across a list of dfs and make the year and quarter updates. We'll then only need to apply additional fixes to q4_2022, q4_2021, q1_2020 and q3_2018.
q4_2022 <- read_csv("data/2022Q4.csv") %>%
mutate(YEAR = as.numeric(2022)) %>%
clean_names() %>%
mutate(quarter = as.numeric("4")) %>%
rename(start_date = perform_start_dt,
end_date = perform_end_dt)
q3_2022 <- read_csv("data/2022Q3.csv") %>%
mutate(YEAR = as.numeric(2022)) %>%
clean_names() %>%
mutate(quarter = as.numeric("3"))
q2_2022 <- read_csv("data/2022Q2.csv") %>%
mutate(YEAR = as.numeric(2022)) %>%
clean_names() %>%
mutate(quarter = as.numeric("2"))
q1_2022 <- read_csv("data/2022Q1.csv")%>%
mutate(YEAR = as.numeric(2022)) %>%
clean_names() %>%
mutate(quarter = as.numeric("1"))
# Need to populate the QUARTER column
q4_2021 <- read_csv("data/2021Q4.csv") %>%
mutate(YEAR = as.numeric(2021)) %>%
clean_names() %>%
mutate(quarter = as.numeric("4"))
# parsing errors in row 27712:
# row   col expected        actual     file
#  <int> <int> <chr>           <chr>      <chr>
#1 27712    12 date in ISO8601 4-Jul-21   ""
#2 27712    14 15 columns      14 columns ""
# Needed to manually add the value 2021-07-04 to q4_2021[27711,12]
q4_2021[27711,12] = as.Date("2021-07-04")
q3_2021 <- read_csv("data/2021Q3.csv") %>%
mutate(YEAR = as.numeric(2021)) %>%
clean_names() %>%
mutate(quarter = as.numeric("3"))
q2_2021 <- read_csv("data/2021Q2.csv")%>%
mutate(YEAR = as.numeric(2021)) %>%
clean_names() %>%
mutate(quarter = as.numeric("2"))
# Need to populate the QUARTER column
q1_2021 <- read_csv("data/2021Q1.csv")%>%
mutate(YEAR = as.numeric(2021)) %>%
clean_names() %>%
mutate(quarter = as.numeric("1"))
q4_2020 <- read_csv("data/2020Q4.csv")%>%
mutate(YEAR = as.numeric(2020)) %>%
clean_names() %>%
mutate(quarter = as.numeric("4"))
q3_2020 <- read_csv("data/2020Q3.csv")%>%
mutate(YEAR = as.numeric(2020)) %>%
clean_names() %>%
mutate(quarter = as.numeric("3"))
q2_2020 <- read_csv("data/2020Q2.csv")%>%
mutate(YEAR = as.numeric(2020)) %>%
clean_names() %>%
mutate(quarter = as.numeric("2"))
q1_2020 <- read_csv("data/2020Q1.csv")%>%
mutate(YEAR = as.numeric(2020)) %>%
clean_names() %>%
mutate(quarter = as.numeric("1"))
# parsing errors in :
#      row   col expected actual file
#    <int> <int> <chr>    <chr>  <chr>
# 1 121790    16 a double FISC   ""
# 2 121791    16 a double FISC   ""
# 3 121792    16 a double FISC   ""
# 4 121793    16 a double FISC   ""
# 5 132094    16 a double FISC   ""
# 6 132095    16 a double FISC   ""
# 7 132096    16 a double FISC   ""
# 8 132097    16 a double FISC   ""
# 9 132098    16 a double FISC   ""
#10 132099    16 a double FISC   ""
#11 132100    16 a double FISC   ""
#12 132101    16 a double FISC   ""
#13 132102    16 a double FISC   ""
#14 132103    16 a double FISC   ""
#15 132104    16 a double FISC   ""
#16 132105    16 a double FISC   ""
#17 132106    16 a double FISC   ""
#18 132107    16 a double FISC   ""
#19 132108    16 a double FISC   ""
#20 132109    16 a double FISC   ""
# The parsing issue is that "FISC" is showing up in 20 rows in the YEAR column. R has changed those values to NULL by default.
# No further action required on the parsing issue
# what does the id column here refer to since there's also already a bioguide_id?
q4_2019 <- read_csv("data/2019Q4.csv")%>%
mutate(YEAR = as.numeric(2019)) %>%
clean_names() %>%
mutate(quarter = as.numeric("4"))
q3_2019 <- read_csv("data/2019Q3.csv")%>%
mutate(YEAR = as.numeric(2019)) %>%
clean_names() %>%
mutate(quarter = as.numeric("3"))
q2_2019 <- read_csv("data/2019Q2.csv")%>%
mutate(YEAR = as.numeric(2019)) %>%
clean_names() %>%
mutate(quarter = as.numeric("2"))
# QUARTER col is in format "Q2" rather than just "2"
q1_2019 <- read_csv("data/2019Q1.csv")%>%
mutate(YEAR = as.numeric(2019)) %>%
clean_names() %>%
mutate(quarter = as.numeric("1"))
q4_2018 <- read_csv("data/2018Q4.csv")%>%
mutate(YEAR = as.numeric(2018)) %>%
clean_names() %>%
mutate(quarter = as.numeric("4"))
q3_2018 <- read_csv("data/2018Q3.csv") %>%
mutate(YEAR = as.numeric(2018)) %>%
clean_names() %>%
mutate(quarter = as.numeric("3"))
# Parsing issues:
#    row   col expected        actual      file
#  <int> <int> <chr>           <chr>       <chr>
# 1 90121    11 date in ISO8601 22-AUG-1818 ""
# Need to manually update q3_2018[90120, 11] to 2018-08-22
q3_2018[90120, 11] = as.Date("2018-08-22")
q2_2018 <- read_csv("data/2018Q2.csv")%>%
mutate(YEAR = as.numeric(2018)) %>%
clean_names() %>%
mutate(quarter = as.numeric("2"))
q1_2018 <- read_csv("data/2018Q1.csv")%>%
mutate(YEAR = as.numeric("2018")) %>%
clean_names() %>%
mutate(quarter = as.numeric("1"))
# Bind dataframes into one combined dataframe
house_spend_18_23 <- bind_rows(list(q4_2022, q3_2022, q2_2022, q1_2022, q4_2021, q3_2021, q2_2021, q1_2021, q4_2020, q3_2020, q2_2020, q1_2020, q4_2019, q3_2019, q2_2019, q1_2019, q4_2018, q3_2018, q2_2018, q1_2018))
View(house_spend_18_23)
remotes::install_github("mkearney/dapr")
remotes::install_github("mkearney/tfse")
remotes::install_github("mkearney/ppcong")
library(ppcong)
## Save API key for use in ppcong. Data in parentheses can be replaced by another individual API key.
ppc_api_key("yosRYPlksfSYRNTfhgot3bNTzvYQNZ8ztredZ7da", set_renv = TRUE) #this is Aya's api key
# Bring in information on House members for the 115th, 116th and 117th congresses, which cover from January 2017 to January 2023, as our expenditure dataset covers 2018 to 2022.
h115 <- ppc_members(congress = "115", chamber = "house")
h116 <- ppc_members(congress = "116", chamber = "house")
h117 <- ppc_members(congress = "117", chamber = "house")
members_18_22 <- bind_rows(list(h115, h116, h117))
View(members_18_22)
cleaned_members_18_22 <- members_18_22 |>
select(id, first_name, middle_name, last_name, gender, party, state, district) |>
distinct()
View(cleaned_members_18_22)
cleaned_members_18_22 <- members_18_22 |>
select(id, first_name, last_name, gender, party, state, district) |>
distinct()
cleaned_members_18_22 <- members_18_22 |>
select(id, first_name, last_name, gender, party, state, district) |>
distinct()|>
str_to_upper(first_name, last_name)
cleaned_members_18_22 <- members_18_22 |>
select(id, first_name, last_name, gender, party, state, district) |>
distinct()|>
mutate(first_name = str_to_upper(first_name))
cleaned_members_18_22 <- members_18_22 |>
select(id, first_name, last_name, gender, party, state, district) |>
distinct()|>
mutate(first_name = str_to_upper(first_name))|>
mutate(last_name = str_to_upper(last_name))
spend_with_member_info <- house_spend_18_22 |>
left_join(cleaned_members_18_22, by = c("bioguide_id" = "id"))
spend_with_member_info <- house_spend_18_23 |>
left_join(cleaned_members_18_22, by = c("bioguide_id" = "id"))
View(spend_with_member_info)
View(cleaned_members_18_22)
View(cleaned_members_18_22)
View(cleaned_members_18_22)
View(cleaned_members_18_22)
View(cleaned_members_18_22)
View(cleaned_members_18_22)
View(cleaned_members_18_22)
View(cleaned_members_18_22)
View(cleaned_members_18_22)
knitr::opts_chunk$set(echo = TRUE)
# Turn off scientific notation.
options(scipen=999)
# Load libraries.
library(tidyverse)
library(readr)
library(scales)
library(janitor)
# Load the data from ProPublica.
# Watch out for "subtotal" rows -- need to filter those out when doing calculations to avoid doubling the real total
# "DATE" column seems to pertain to the date the expense was filed, whereas the start and end date cols seem to pertain to the dates of the expenses themselves. for example, q4_2019 has expenses that go back all the way to 2011
# YEAR is not always populated. When it is, it seems to pertain to the filing year rather than the expense year.
# QUARTER is sometimes unpopulated, sometimes populated with a numeric, and sometimes populated with a chr (eg "Q2" instead of just "2").
# DATE col values aren't always populated (within the same df some rows are and some rows aren't). Can't use it to extract quarter and year values.
# Cleaned datatypes in columns one-by-one while exploring the distinct issues across different quarters. We will make this more concise in the near future by using regex and a for loop to iterate across a list of dfs and make the year and quarter updates. We'll then only need to apply additional fixes to q4_2022, q4_2021, q1_2020 and q3_2018.
q4_2022 <- read_csv("data/2022Q4.csv") %>%
mutate(YEAR = as.numeric(2022)) %>%
clean_names() %>%
mutate(quarter = as.numeric("4")) %>%
rename(start_date = perform_start_dt,
end_date = perform_end_dt)
q3_2022 <- read_csv("data/2022Q3.csv") %>%
mutate(YEAR = as.numeric(2022)) %>%
clean_names() %>%
mutate(quarter = as.numeric("3"))
q2_2022 <- read_csv("data/2022Q2.csv") %>%
mutate(YEAR = as.numeric(2022)) %>%
clean_names() %>%
mutate(quarter = as.numeric("2"))
q1_2022 <- read_csv("data/2022Q1.csv")%>%
mutate(YEAR = as.numeric(2022)) %>%
clean_names() %>%
mutate(quarter = as.numeric("1"))
# Need to populate the QUARTER column
q4_2021 <- read_csv("data/2021Q4.csv") %>%
mutate(YEAR = as.numeric(2021)) %>%
clean_names() %>%
mutate(quarter = as.numeric("4"))
# parsing errors in row 27712:
# row   col expected        actual     file
#  <int> <int> <chr>           <chr>      <chr>
#1 27712    12 date in ISO8601 4-Jul-21   ""
#2 27712    14 15 columns      14 columns ""
# Needed to manually add the value 2021-07-04 to q4_2021[27711,12]
q4_2021[27711,12] = as.Date("2021-07-04")
q3_2021 <- read_csv("data/2021Q3.csv") %>%
mutate(YEAR = as.numeric(2021)) %>%
clean_names() %>%
mutate(quarter = as.numeric("3"))
q2_2021 <- read_csv("data/2021Q2.csv")%>%
mutate(YEAR = as.numeric(2021)) %>%
clean_names() %>%
mutate(quarter = as.numeric("2"))
# Need to populate the QUARTER column
q1_2021 <- read_csv("data/2021Q1.csv")%>%
mutate(YEAR = as.numeric(2021)) %>%
clean_names() %>%
mutate(quarter = as.numeric("1"))
q4_2020 <- read_csv("data/2020Q4.csv")%>%
mutate(YEAR = as.numeric(2020)) %>%
clean_names() %>%
mutate(quarter = as.numeric("4"))
q3_2020 <- read_csv("data/2020Q3.csv")%>%
mutate(YEAR = as.numeric(2020)) %>%
clean_names() %>%
mutate(quarter = as.numeric("3"))
q2_2020 <- read_csv("data/2020Q2.csv")%>%
mutate(YEAR = as.numeric(2020)) %>%
clean_names() %>%
mutate(quarter = as.numeric("2"))
q1_2020 <- read_csv("data/2020Q1.csv")%>%
mutate(YEAR = as.numeric(2020)) %>%
clean_names() %>%
mutate(quarter = as.numeric("1"))
# parsing errors in :
#      row   col expected actual file
#    <int> <int> <chr>    <chr>  <chr>
# 1 121790    16 a double FISC   ""
# 2 121791    16 a double FISC   ""
# 3 121792    16 a double FISC   ""
# 4 121793    16 a double FISC   ""
# 5 132094    16 a double FISC   ""
# 6 132095    16 a double FISC   ""
# 7 132096    16 a double FISC   ""
# 8 132097    16 a double FISC   ""
# 9 132098    16 a double FISC   ""
#10 132099    16 a double FISC   ""
#11 132100    16 a double FISC   ""
#12 132101    16 a double FISC   ""
#13 132102    16 a double FISC   ""
#14 132103    16 a double FISC   ""
#15 132104    16 a double FISC   ""
#16 132105    16 a double FISC   ""
#17 132106    16 a double FISC   ""
#18 132107    16 a double FISC   ""
#19 132108    16 a double FISC   ""
#20 132109    16 a double FISC   ""
# The parsing issue is that "FISC" is showing up in 20 rows in the YEAR column. R has changed those values to NULL by default.
# No further action required on the parsing issue
# what does the id column here refer to since there's also already a bioguide_id?
q4_2019 <- read_csv("data/2019Q4.csv")%>%
mutate(YEAR = as.numeric(2019)) %>%
clean_names() %>%
mutate(quarter = as.numeric("4"))
q3_2019 <- read_csv("data/2019Q3.csv")%>%
mutate(YEAR = as.numeric(2019)) %>%
clean_names() %>%
mutate(quarter = as.numeric("3"))
q2_2019 <- read_csv("data/2019Q2.csv")%>%
mutate(YEAR = as.numeric(2019)) %>%
clean_names() %>%
mutate(quarter = as.numeric("2"))
# QUARTER col is in format "Q2" rather than just "2"
q1_2019 <- read_csv("data/2019Q1.csv")%>%
mutate(YEAR = as.numeric(2019)) %>%
clean_names() %>%
mutate(quarter = as.numeric("1"))
q4_2018 <- read_csv("data/2018Q4.csv")%>%
mutate(YEAR = as.numeric(2018)) %>%
clean_names() %>%
mutate(quarter = as.numeric("4"))
q3_2018 <- read_csv("data/2018Q3.csv") %>%
mutate(YEAR = as.numeric(2018)) %>%
clean_names() %>%
mutate(quarter = as.numeric("3"))
# Parsing issues:
#    row   col expected        actual      file
#  <int> <int> <chr>           <chr>       <chr>
# 1 90121    11 date in ISO8601 22-AUG-1818 ""
# Need to manually update q3_2018[90120, 11] to 2018-08-22
q3_2018[90120, 11] = as.Date("2018-08-22")
q2_2018 <- read_csv("data/2018Q2.csv")%>%
mutate(YEAR = as.numeric(2018)) %>%
clean_names() %>%
mutate(quarter = as.numeric("2"))
q1_2018 <- read_csv("data/2018Q1.csv")%>%
mutate(YEAR = as.numeric("2018")) %>%
clean_names() %>%
mutate(quarter = as.numeric("1"))
# Bind dataframes into one combined dataframe
house_spend_18_23 <- bind_rows(list(q4_2022, q3_2022, q2_2022, q1_2022, q4_2021, q3_2021, q2_2021, q1_2021, q4_2020, q3_2020, q2_2020, q1_2020, q4_2019, q3_2019, q2_2019, q1_2019, q4_2018, q3_2018, q2_2018, q1_2018))
# Install necessary packages (skip this step if packages have been previously installed)
remotes::install_github("mkearney/dapr")
remotes::install_github("mkearney/tfse")
remotes::install_github("mkearney/ppcong")
# Load ppcong library
library(ppcong)
## Save API key for use in ppcong. Data in parentheses can be replaced by another individual API key.
ppc_api_key("yosRYPlksfSYRNTfhgot3bNTzvYQNZ8ztredZ7da", set_renv = TRUE) #this is Aya's api key
# Bring in information on House members for the 115th, 116th and 117th congresses, which cover from January 2017 to January 2023, as our expenditure dataset covers 2018 to 2022.
h115 <- ppc_members(congress = "115", chamber = "house")
h116 <- ppc_members(congress = "116", chamber = "house")
h117 <- ppc_members(congress = "117", chamber = "house")
setwd("~/Data Journalism/data_analysis_proj")
# Bind dataframes from the three Congresses
members_18_22 <- bind_rows(list(h115, h116, h117))
View(members_18_22)
cleaned_members_18_22 <- members_18_22 |>
select(id, first_name, last_name, gender, party, state, district) |>
distinct()|>
mutate(first_name = str_to_upper(first_name))|>
mutate(last_name = str_to_upper(last_name))
View(cleaned_members_18_22)
spend_with_member_info <- house_spend_18_23 |>
left_join(cleaned_members_18_22, by = c("bioguide_id" = "id"))
View(spend_with_member_info)
write_csv(spend_with_member_info, "data/spend_with_member_info.csv")
View(spend_with_member_info)
View(house_spend_18_23)
write_csv(house_spend_18_23, "data/house_spend_18_23.csv")
View(max_years)
write_csv(house_spend_18_23, "data/house_spend_18_23.csv")
write_csv(house_spend_18_23, "data/house_spend_18_23.csv")
